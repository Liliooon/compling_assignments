{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry18I7w2Hk77"
   },
   "source": [
    "# Разные способы получить ключевые слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так получилось, что я делала похожее задание раньше, так что я просто сдам вам эту тетрадку. Тут неправильно реализован индекс Жаккара и некоторые вещи я бы сейчас реализовала лучше, но, к сожалению, времени или сил править что-либо в нет. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "751iBgFnHk78"
   },
   "source": [
    "### Подготовка тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dDyuzTUSHk79"
   },
   "source": [
    "Получаем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:42:35.460889Z",
     "start_time": "2020-04-27T05:42:35.456901Z"
    },
    "hidden": true,
    "id": "Xxhib81kHk7-"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:42:35.802975Z",
     "start_time": "2020-04-27T05:42:35.797987Z"
    },
    "hidden": true,
    "id": "m17cllTcHk8B"
   },
   "outputs": [],
   "source": [
    "PATH = 'samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:42:36.713700Z",
     "start_time": "2020-04-27T05:42:36.707716Z"
    },
    "hidden": true,
    "id": "rNxX8e8OHk8E"
   },
   "outputs": [],
   "source": [
    "def get_texts(path):\n",
    "    \"\"\"берет путь к папке с текстами и извлекает тексты в виде строк\"\"\"\n",
    "    texts = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:42:37.801225Z",
     "start_time": "2020-04-27T05:42:37.793249Z"
    },
    "hidden": true,
    "id": "0D-GMv1vHk8H"
   },
   "outputs": [],
   "source": [
    "texts = get_texts(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "--r-FH0iHk8J"
   },
   "source": [
    "Получаем лемматизированные с помощью MyStem тексты без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:42:40.251319Z",
     "start_time": "2020-04-27T05:42:40.246333Z"
    },
    "hidden": true,
    "id": "YE_ihm32Hk8K"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:42:41.464692Z",
     "start_time": "2020-04-27T05:42:41.459704Z"
    },
    "hidden": true,
    "id": "hdKitokrHk8N"
   },
   "outputs": [],
   "source": [
    "def get_mystem_lemmatized_text(text):\n",
    "    \"\"\"\n",
    "    берет текст в виде строки и возвращает его \n",
    "    лемматизированным MyStem в виде списка слов\n",
    "    \"\"\"\n",
    "    stop_words = set(get_stop_words('ru'))\n",
    "    stop_words.update(['чей', 'свой', 'ежели', 'нешто', 'из-за'])\n",
    "    lemmatized_text = [word for word in m.lemmatize(text)\n",
    "                       if re.search('[а-яёa-z]', word)\n",
    "                       and word not in stop_words]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "F8HsyiRyHk8Q"
   },
   "outputs": [],
   "source": [
    "mystem_lemmatized_texts = [get_mystem_lemmatized_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ynZfPfQZHk8S"
   },
   "source": [
    "Получаем лемматизированные с помощью pymorphy2 тексты без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "3_wzUYXPHk8T"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation \n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "VuYvUFxKHk8V"
   },
   "outputs": [],
   "source": [
    "def get_pymorphy2_lemmatized_text(text):\n",
    "    \"\"\"\n",
    "    берет текст в виде строки и возвращает его без стоп-слов и\n",
    "    лемматизированным pymorphy2 в виде списка слов\n",
    "    \"\"\"\n",
    "    stop_words = set(get_stop_words('ru'))\n",
    "    stop_words.update(['чей', 'свой', 'ежели', 'нешто', 'из-за'])\n",
    "    lemmatized_text = [morph.parse(word)[0].normal_form.strip(punctuation+'.— ') \n",
    "                       for word in word_tokenize(text)\n",
    "                       if re.search('[а-яёa-z]', word, flags=re.I)\n",
    "                       and morph.parse(word)[0].normal_form.strip(punctuation+'.— ') \n",
    "                       not in stop_words]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "qcLqVg-fHk8Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pymorphy2_lemmatized_texts = [get_pymorphy2_lemmatized_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "_dv_1xV-Hk8b"
   },
   "source": [
    "### 7 способов извлечения ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "L1ye-w8XHk8c"
   },
   "source": [
    "Все способы подразумевают применение на лемматизированном тексте без стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "SaopXWjWHk8d"
   },
   "source": [
    "#### 1. Ключевые слова по частотности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Nwd7qktfHk8d"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "0vbda-5GHk8g"
   },
   "outputs": [],
   "source": [
    "def get_keywords_most_frequent(text, num_keywords):\n",
    "    \"\"\"\n",
    "    берет текст в виде списка слов и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов самые частотные слова текста\n",
    "    \"\"\"\n",
    "    keywords = [word_freq[0] for word_freq in Counter(text).most_common(num_keywords)]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "P41R4SA-Hk8i",
    "outputId": "dba1e273-efb9-49a5-8720-e0192848863b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гайка', 'денис', 'грузило', 'понимать', 'следователь']\n",
      "['иван', 'пугачев', 'савельй', 'отвечать', 'марья']\n",
      "['колобок', 'уйти', 'короб', 'сусек', 'заяц']\n",
      "['тест', 'тестирование', 'лаборатория', 'роспотребнадзор', 'коронавирус']\n",
      "['неделя', 'расход', 'россиянин', 'категория', 'товар']\n"
     ]
    }
   ],
   "source": [
    "for text in pymorphy2_lemmatized_texts:\n",
    "    print(get_keywords_most_frequent(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "BrOgG-UbHk8m",
    "outputId": "0fd400d2-d746-4ffa-b0f4-97d575fe64db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['денис', 'гайка', 'отвинчивать', 'грузило', 'понимать']\n",
      "['пугачев', 'марья', 'ивановна', 'савельич', 'отвечать']\n",
      "['колобок', 'уходить', 'лиса', 'короб', 'сусек']\n",
      "['тестирование', 'лаборатория', 'тест', 'роспотребнадзор', 'коронавирус']\n",
      "['неделя', 'расход', 'россиянин', 'категория', 'товар']\n"
     ]
    }
   ],
   "source": [
    "for text in mystem_lemmatized_texts:\n",
    "    print(get_keywords_most_frequent(text, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "oj7CdifBHk8q"
   },
   "source": [
    "#### 2. Ключевые слова и биграммы по частотности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "GQJHUTL9Hk8q"
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4wwnlTEsHk8t"
   },
   "outputs": [],
   "source": [
    "def get_keywords_bigrams_most_frequent(text, num_keywords):\n",
    "    \"\"\"\n",
    "    берет текст в виде списка слов и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов самые частотные биграммы текста, \n",
    "    согласованные грамматически, и самые частотные слова \n",
    "    в зависимости от их нормированной частотности\n",
    "    \"\"\"\n",
    "    bigrams = [(text[i], text[i + 1]) for i, word in enumerate(text) if i != (len(text) - 1)]\n",
    "    freq_uninflected_bigrams_numbers = dict([bigram_freq for bigram_freq in Counter(bigrams).most_common(num_keywords)])\n",
    "    freq_words_numbers = dict([(word_freq[0], word_freq[1] / 2) for word_freq in Counter(text).most_common(num_keywords)])\n",
    "    freq_items_numbers = {**freq_uninflected_bigrams_numbers , **freq_words_numbers}\n",
    "    freq_items_numbers = sorted(freq_items_numbers.items(), key=lambda item: item[1], reverse=True)[:num_keywords]\n",
    "    keywords = []\n",
    "    for item in freq_items_numbers:\n",
    "        if type(item[0]) == tuple: \n",
    "            new_bigram = None\n",
    "            first_word = morph.parse(item[0][0])[0]\n",
    "            second_word = morph.parse(item[0][1])[0]\n",
    "            if 'ADJF' in first_word.tag or 'ADJS' in first_word.tag:\n",
    "                if second_word.tag.gender:\n",
    "                    new_bigram = (first_word.inflect({second_word.tag.gender}).word, item[0][1])\n",
    "            elif 'tran' in first_word.tag:\n",
    "                if 'NOUN' in second_word.tag or 'ADJF' in first_word.tag or 'ADJS' in first_word.tag:\n",
    "                    new_bigram = (item[0][0], second_word.inflect({'accs'}).word)\n",
    "            elif 'intr' in first_word.tag and 'NOUN' in second_word.tag:\n",
    "                if 'NOUN' in second_word.tag or 'ADJF' in first_word.tag or 'ADJS' in first_word.tag:\n",
    "                    new_bigram = (item[0][0], second_word.inflect({'ablt'}).word)\n",
    "            elif 'NOUN' in first_word.tag and 'NOUN' in second_word.tag:\n",
    "                if 'Surn' not in first_word.tag and 'Name' not in first_word.tag:\n",
    "                    new_bigram = (item[0][0], second_word.inflect({'gent'}).word)\n",
    "            if new_bigram:\n",
    "                keywords.append(' '.join(new_bigram))\n",
    "            else:\n",
    "                keywords.append(' '.join(item[0]))\n",
    "        else:\n",
    "             keywords.append(item[0]) \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "PnB_HrZPHk80",
    "outputId": "472c1f00-9ddb-4266-cbf8-332dfa78e344",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гайка', 'денис', 'грузило', 'понимать', 'следователь']\n",
      "['иван', 'марья иван', 'пугачев', 'иван кузмич', 'савельй']\n",
      "['колобок', 'уйти', 'колобок колобка', 'сказка колобка', 'колобок картинки']\n",
      "['тест', 'тестирование', 'лаборатория', 'роспотребнадзор', 'тестирование коронавируса']\n",
      "['неделя', 'салон красоты', 'расход', 'сократить расход', 'ювелирное изделие']\n"
     ]
    }
   ],
   "source": [
    "for text in pymorphy2_lemmatized_texts:\n",
    "    print(get_keywords_bigrams_most_frequent(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RZe7SQANHk83",
    "outputId": "588f036a-6b3e-4869-b31c-63c39d166f99",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['денис', 'гайка', 'отвинчивать гайку', 'отвинчивать', 'грузило']\n",
      "['марья ивановна', 'пугачев', 'марья', 'ивановна', 'иван кузмич']\n",
      "['колобок', 'уходить', 'колобок колобка', 'сказка колобка', 'колобок картинки']\n",
      "['тестирование', 'лаборатория', 'тест', 'роспотребнадзор', 'тестирование коронавируса']\n",
      "['неделя', 'салон красоты', 'расход', 'сокращать расход', 'ювелирное изделие']\n"
     ]
    }
   ],
   "source": [
    "for text in mystem_lemmatized_texts:\n",
    "    print(get_keywords_bigrams_most_frequent(text, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "CCpmCA5oHk85"
   },
   "source": [
    "#### 3. Ключевые слова и биграммы по частотности и без глаголов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "u5OY8iBPHk85"
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:36:57.005297Z",
     "start_time": "2020-04-27T09:36:56.973378Z"
    },
    "hidden": true,
    "id": "IWuInmA7Hk89"
   },
   "outputs": [],
   "source": [
    "def get_keywords_bigrams_most_frequent_without_verbs(text, num_keywords):\n",
    "    \"\"\"\n",
    "    берет текст в виде списка слов и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов самые частотные биграммы текста, \n",
    "    согласованные грамматически и не содержащие глаголы, и самые частотные слова, \n",
    "    не являющиеся глаголами, в зависимости от их нормированной частотности\n",
    "    \"\"\"\n",
    "    bigrams = [(text[i], text[i + 1]) for i, word in enumerate(text) if i != (len(text) - 1)]\n",
    "    freq_uninflected_bigrams_numbers = dict([bigram_freq for bigram_freq in Counter(bigrams).most_common()])\n",
    "    freq_words_numbers = dict([(word_freq[0], word_freq[1] / 2) for word_freq in Counter(text).most_common()])\n",
    "    freq_items_numbers = list({**freq_uninflected_bigrams_numbers , **freq_words_numbers}.items())\n",
    "    items_to_remove = []\n",
    "    for item in freq_items_numbers:\n",
    "        if type(item[0]) == str:\n",
    "            word = morph.parse(item[0])[0]\n",
    "            if 'INFN' in word.tag:\n",
    "                items_to_remove.append(item)\n",
    "        else: \n",
    "            first_word = morph.parse(item[0][0])[0]\n",
    "            second_word = morph.parse(item[0][1])[0]\n",
    "            if 'INFN' in first_word.tag or 'INFN' in second_word.tag:\n",
    "                items_to_remove.append(item)\n",
    "    freq_items_numbers = [item for item in freq_items_numbers if item not in items_to_remove]\n",
    "    freq_items_numbers = sorted(freq_items_numbers, key=lambda item: item[1], reverse=True)[:num_keywords]\n",
    "    keywords = []\n",
    "    for item in freq_items_numbers:\n",
    "        if type(item[0]) == tuple: \n",
    "            new_bigram = None\n",
    "            first_word = morph.parse(item[0][0])[0]\n",
    "            second_word = morph.parse(item[0][1])[0]\n",
    "            if 'ADJF' in first_word.tag or 'ADJS' in first_word.tag:\n",
    "                if second_word.tag.gender:\n",
    "                    new_bigram = (first_word.inflect({second_word.tag.gender}).word, item[0][1])\n",
    "            elif 'NOUN' in first_word.tag and 'NOUN' in second_word.tag:\n",
    "                if second_word.inflect({'gent'}):\n",
    "                    if 'Surn' not in first_word.tag and 'Name' not in first_word.tag:\n",
    "                        new_bigram = (item[0][0], second_word.inflect({'gent'}).word)\n",
    "            if new_bigram:\n",
    "                keywords.append(' '.join(new_bigram))\n",
    "            else:\n",
    "                keywords.append(' '.join(item[0]))\n",
    "        else:\n",
    "             keywords.append(item[0]) \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:37:49.532236Z",
     "start_time": "2020-04-27T09:37:01.798881Z"
    },
    "hidden": true,
    "id": "4XG6SfC9Hk8_",
    "outputId": "76818846-d67f-49c9-8ffc-2d12c2e20097",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гайка', 'денис', 'грузило', 'следователь', 'рельс']\n",
      "['иван', 'марья иван', 'пугачев', 'иван кузмич', 'савельй']\n",
      "['колобок', 'колобок колобка', 'сказка колобка', 'колобок картинки', 'колобок короба']\n",
      "['тест', 'тестирование', 'лаборатория', 'роспотребнадзор', 'тестирование коронавируса']\n",
      "['неделя', 'салон красоты', 'расход', 'ювелирное изделие', 'уровень потребления']\n"
     ]
    }
   ],
   "source": [
    "for text in pymorphy2_lemmatized_texts:\n",
    "    print(get_keywords_bigrams_most_frequent_without_verbs(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:38:40.855120Z",
     "start_time": "2020-04-27T09:37:50.280298Z"
    },
    "hidden": true,
    "id": "lCfe0BDDHk9B",
    "outputId": "e6484cf4-2880-482d-ea1c-87354b8e5289",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['денис', 'гайка', 'грузило', 'следователь', 'рельс']\n",
      "['марья ивановна', 'пугачев', 'марья', 'ивановна', 'иван кузмич']\n",
      "['колобок', 'колобок колобка', 'сказка колобка', 'колобок картинки', 'колобок короба']\n",
      "['тестирование', 'лаборатория', 'тест', 'роспотребнадзор', 'тестирование коронавируса']\n",
      "['неделя', 'салон красоты', 'расход', 'ювелирное изделие', 'аналитик сбербанка']\n"
     ]
    }
   ],
   "source": [
    "for text in mystem_lemmatized_texts:\n",
    "    print(get_keywords_bigrams_most_frequent_without_verbs(text, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "utu9-t9iHk9D"
   },
   "source": [
    "#### 4. Ключевые слова по TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "dIPoxCOIHk9E"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "MZGaQ3LhHk9H"
   },
   "outputs": [],
   "source": [
    "def get_keywords_tf_idf(texts, num_keywords):\n",
    "    \"\"\"\n",
    "    берет список текстов в виде строк или списков\n",
    "    и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов слова, отобранные методом TF-IDF\n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "    make_tf_idf = TfidfVectorizer()\n",
    "    for text in texts:\n",
    "        if type(text) == list:\n",
    "            texts_as_tfidf_vectors = make_tf_idf.fit_transform(' '.join(text) for text in texts)\n",
    "        elif type(text) == str:\n",
    "            texts_as_tfidf_vectors = make_tf_idf.fit_transform(texts)\n",
    "    id2word = {i:word for i, word in enumerate(make_tf_idf.get_feature_names())} \n",
    "    for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "        row_data = texts_as_tfidf_vectors.getrow(text_row) \n",
    "        words_for_this_text = row_data.toarray().argsort() \n",
    "        top_words_for_this_text = words_for_this_text [0,:-(num_keywords+1):-1] \n",
    "        keywords_for_this_text = [id2word[w] for w in top_words_for_this_text]\n",
    "        keywords.append(keywords_for_this_text)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "llSgG9RdHk9J",
    "outputId": "d0cdf713-7b1e-49e7-c3c1-609c73058edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гайка', 'денис', 'грузило', 'следователь', 'понимать']\n",
      "['иван', 'пугачев', 'марья', 'савельй', 'крепость']\n",
      "['колобок', 'уйти', 'короб', 'заяц', 'сусек']\n",
      "['тест', 'тестирование', 'роспотребнадзор', 'лаборатория', 'коронавирус']\n",
      "['неделя', 'расход', 'товар', 'россиянин', 'категория']\n"
     ]
    }
   ],
   "source": [
    "for text_keywords in get_keywords_tf_idf(pymorphy2_lemmatized_texts, 5):\n",
    "    print(text_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "_b7K_Ky1Hk9L",
    "outputId": "eecc0be5-fbe1-456f-bcef-ce3b6139f640",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['денис', 'гайка', 'грузило', 'отвинчивать', 'понимать']\n",
      "['пугачев', 'марья', 'ивановна', 'савельич', 'крепость']\n",
      "['колобок', 'уходить', 'лиса', 'сусек', 'короб']\n",
      "['тест', 'тестирование', 'роспотребнадзор', 'лаборатория', 'коронавирус']\n",
      "['неделя', 'расход', 'товар', 'категория', 'россиянин']\n"
     ]
    }
   ],
   "source": [
    "for text_keywords in get_keywords_tf_idf(mystem_lemmatized_texts, 5):\n",
    "    print(text_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "BNW5NAUgHk9O"
   },
   "source": [
    "#### 5. Ключевые слова и биграммы по TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fWxofwwfHk9O"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "DyL6n2pxHk9Q"
   },
   "outputs": [],
   "source": [
    "rus_stop_words = get_stop_words('ru')\n",
    "rus_stop_words.extend(['чей', 'свой', 'ежели', 'нешто', 'из-за'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "XOJyo3xrHk9T"
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_text(text):\n",
    "    \"\"\"\n",
    "    берет текст в виде строки и возвращает его без стоп-слов и\n",
    "    лемматизированным pymorphy2 в виде списка слов\n",
    "    \"\"\"\n",
    "    preprocessed_text = ' '.join([morph.parse(word)[0].normal_form.strip(punctuation+'.— ') \n",
    "                       for word in word_tokenize(text)\n",
    "                       if re.search('[а-яёa-z]', word, flags=re.I)])\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "zWdgiLmQHk9V"
   },
   "outputs": [],
   "source": [
    "def get_keywords_bigrams_tf_idf(texts, num_keywords):\n",
    "    \"\"\"\n",
    "    берет список не лемматизированных текстов в виде строк\n",
    "    и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов слова и биграммы, \n",
    "    отобранные методом TF-IDF\n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "    make_tf_idf = TfidfVectorizer(stop_words=rus_stop_words, ngram_range=(1, 2))\n",
    "    texts_as_tfidf_vectors = make_tf_idf.fit_transform(get_preprocessed_text(text) for text in texts)\n",
    "    id2word = {i : word for i, word in enumerate(make_tf_idf.get_feature_names())} \n",
    "    for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "        row_data = texts_as_tfidf_vectors.getrow(text_row) \n",
    "        words_for_this_text = row_data.toarray().argsort() \n",
    "        top_words_for_this_text = words_for_this_text [0, : -(num_keywords + 1) : -1] \n",
    "        keywords_for_this_text = [id2word[w] for w in top_words_for_this_text]\n",
    "        keywords.append(keywords_for_this_text)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "frstZRaAHk9X",
    "outputId": "d789c86d-b799-4d02-b4d7-c14a401c638a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гайка', 'денис', 'грузило', 'следователь', 'понимать']\n",
      "['иван', 'пугачев', 'марья', 'савельй', 'марья иван']\n",
      "['колобок', 'уйти', 'колобок колобок', 'короб', 'лис']\n",
      "['тест', 'тестирование', 'роспотребнадзор', 'лаборатория', 'коронавирус']\n",
      "['неделя', 'расход', 'россиянин', 'товар', 'категория']\n"
     ]
    }
   ],
   "source": [
    "for text_keywords in get_keywords_bigrams_tf_idf(texts, 5):\n",
    "    print(text_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "HyhmW5k3Hk9Z"
   },
   "source": [
    "#### 6. Ключевые слова по degree centrality графа совместной встречаемости слов в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "2aYNLZdjHk9a"
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "f0lVv7LuHk9c"
   },
   "outputs": [],
   "source": [
    "def get_keywords_graph_degree_centrality(text, num_keywords):\n",
    "    \"\"\"\n",
    "    берет текст в виде списка слов и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов узлы графа с самой высокой степенью degree centrality\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from([(text[i], text[i+1]) for i, item in enumerate(text) if i != (len(text)-1)])\n",
    "    dc = nx.degree_centrality(G)\n",
    "    keywords = [node for node in sorted(dc, key=dc.get, reverse=True)[:num_keywords]]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CiXuTLf0Hk9e",
    "outputId": "f9e7f1e7-43f6-4633-a617-a8a4567969fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гайка', 'денис', 'грузило', 'понимать', 'следователь']\n",
      "['пугачев', 'иван', 'савельй', 'отвечать', 'швабрин']\n",
      "['колобок', 'лис', 'старуха', 'уйти', 'волк']\n",
      "['тест', 'лаборатория', 'роспотребнадзор', 'тестирование', 'биоматериал']\n",
      "['неделя', 'расход', 'россиянин', 'категория', 'товар']\n"
     ]
    }
   ],
   "source": [
    "for text in pymorphy2_lemmatized_texts:\n",
    "    print(get_keywords_graph_degree_centrality(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "kmc91YY1Hk9g",
    "outputId": "5cb30f22-83c3-4a23-86e4-602e7e6c66dd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['денис', 'гайка', 'грузило', 'понимать', 'отвинчивать']\n",
      "['пугачев', 'савельич', 'отвечать', 'крепость', 'становиться']\n",
      "['колобок', 'лиса', 'старуха', 'уходить', 'волк']\n",
      "['лаборатория', 'тест', 'роспотребнадзор', 'тестирование', 'биоматериал']\n",
      "['неделя', 'расход', 'россиянин', 'категория', 'товар']\n"
     ]
    }
   ],
   "source": [
    "for text in mystem_lemmatized_texts:\n",
    "    print(get_keywords_graph_degree_centrality(text, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "mkbQmAfTHk9j"
   },
   "source": [
    "#### 7. Ключевые слова по betweenness centrality графа совместной встречаемости слов в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "27igmPB7Hk9k"
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "WZ24jZj7Hk9m"
   },
   "outputs": [],
   "source": [
    "def get_keywords_graph_betweenness_centrality(text, num_keywords):\n",
    "    \"\"\"\n",
    "    берет текст в виде списка слов и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов узлы графа с самой высокой степенью degree centrality\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from([(text[i], text[i+1]) for i, item in enumerate(text) if i != (len(text)-1)])\n",
    "    bc = nx.betweenness_centrality(G)\n",
    "    keywords = [node for node in sorted(bc, key=bc.get, reverse=True)[:num_keywords]]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "DWZ67gRaHk9q",
    "outputId": "0a4d69a7-f988-4206-937e-36794299338e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гайка', 'денис', 'грузило', 'следователь', 'понимать']\n",
      "['пугачев', 'иван', 'савельй', 'отвечать', 'швабрин']\n",
      "['колобок', 'старуха', 'далёкий', 'короб', 'уйти']\n",
      "['лаборатория', 'тест', 'роспотребнадзор', 'коронавирус', 'тестирование']\n",
      "['неделя', 'расход', 'сбербанк', 'россиянин', 'категория']\n"
     ]
    }
   ],
   "source": [
    "for text in pymorphy2_lemmatized_texts:\n",
    "    print(get_keywords_graph_betweenness_centrality(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "uzaygg2tHk9s",
    "outputId": "871d77cb-5c42-4786-88e3-77b598c37e43",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['денис', 'грузило', 'гайка', 'понимать', 'следователь']\n",
      "['пугачев', 'савельич', 'отвечать', 'крепость', 'швабрин']\n",
      "['колобок', 'старуха', 'лиса', 'уходить', 'короб']\n",
      "['лаборатория', 'роспотребнадзор', 'тест', 'частный', 'тестирование']\n",
      "['неделя', 'расход', 'сбербанк', 'трата', 'категория']\n"
     ]
    }
   ],
   "source": [
    "for text in mystem_lemmatized_texts:\n",
    "    print(get_keywords_graph_betweenness_centrality(text, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fWTvZfYHk9v"
   },
   "source": [
    "### Оценка способов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:43:24.866160Z",
     "start_time": "2020-04-27T05:43:24.863165Z"
    },
    "id": "6PCJO_TDHk9v"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEMLV7gCHk9x"
   },
   "source": [
    "Возьмем один из доступных корпусов с метаданными в JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:43:26.194902Z",
     "start_time": "2020-04-27T05:43:26.189914Z"
    },
    "id": "Ah6s1SBaHk9y"
   },
   "outputs": [],
   "source": [
    "def get_corpus(corpus_name):\n",
    "    \"\"\"\n",
    "    берет в формате строки название корпуса, существующего в формате jsonlines, \n",
    "    и возвращает его в виде списка словарей\n",
    "    \"\"\"\n",
    "    with open(f'data/{corpus_name}.jsonlines', 'r', encoding='utf-8') as f:\n",
    "        corpus_name = [json.loads(line) for line in f]\n",
    "        return corpus_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:55:41.382349Z",
     "start_time": "2020-04-27T05:55:41.350402Z"
    },
    "id": "VsUWRng7Hk91"
   },
   "outputs": [],
   "source": [
    "corpus = get_corpus('russia_today_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X83hugBOHk95"
   },
   "source": [
    "Извлекаем все тексты корпуса в список, где каждый текст -- строка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:55:46.525393Z",
     "start_time": "2020-04-27T05:55:46.494479Z"
    },
    "id": "mb0YllrKHk96",
    "outputId": "b1f5ec62-94e7-4ec0-a919-f124babb6ed8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 текстов\n",
      "207852 слов\n"
     ]
    }
   ],
   "source": [
    "corpus_texts = [item['content'] for item in corpus if item['content']] \n",
    "print(f'{len(corpus_texts)} текстов')\n",
    "print(f'{sum(len(text.split()) for text in corpus_texts)} слов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B6UXKdSHk99"
   },
   "source": [
    "Для каждого текста корпуса достаем написанные человеком т.н. эталонные ключевые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:55:53.266822Z",
     "start_time": "2020-04-27T05:55:53.260839Z"
    },
    "id": "tb_SxgB5Hk9-",
    "outputId": "d18fd151-76ec-4596-e83a-bb1c7cce5753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 групп ключевых слов\n"
     ]
    }
   ],
   "source": [
    "manual_keywords = [item['keywords'] for item in corpus if item['content']] \n",
    "print(f'{len(manual_keywords)} групп ключевых слов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMDwEuVfHk-A"
   },
   "source": [
    "Начинаем сравнивать их с т.н. предсказанными ключевыми словами "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhGevZcJHk-A"
   },
   "source": [
    "Вначале лемматизируем двумя способами корпус текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T05:57:29.535126Z",
     "start_time": "2020-04-27T05:56:01.531152Z"
    },
    "id": "Xjan4CAAHk-A"
   },
   "outputs": [],
   "source": [
    "pymorphy2_lemmatized_corpus_texts = [get_pymorphy2_lemmatized_text(text) for text in corpus_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T08:39:32.409378Z",
     "start_time": "2020-04-27T05:57:29.821715Z"
    },
    "id": "kKSz2S3gHk-C"
   },
   "outputs": [],
   "source": [
    "mystem_lemmatized_corpus_texts = [get_mystem_lemmatized_text(text) for text in corpus_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtudtAS0Hk-E"
   },
   "source": [
    "Применяем все способы (в огромной функции, чтобы можно было менять количество запрашиваемых ключевых слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:31:44.634822Z",
     "start_time": "2020-04-27T09:31:44.613879Z"
    },
    "id": "aqMETU9rHk-F"
   },
   "outputs": [],
   "source": [
    "def get_all_methods_keywords(num_keywords):\n",
    "    \"\"\"\n",
    "    берет количество запрашиваемых ключевых слов в наборе\n",
    "    и возвращает списки наборов ключевых слов\n",
    "    \"\"\"\n",
    "    #1\n",
    "    pymorphy2_predicted_keywords_most_frequent = [get_keywords_most_frequent(text, num_keywords)\n",
    "                                                  for text in pymorphy2_lemmatized_corpus_texts]\n",
    "    #2\n",
    "    pymorphy2_predicted_keywords_bigrams_most_frequent = [get_keywords_bigrams_most_frequent(text, num_keywords)\n",
    "                                                          for text in pymorphy2_lemmatized_corpus_texts]\n",
    "    #3\n",
    "    pymorphy2_predicted_keywords_bigrams_most_frequent_without_verbs = [get_keywords_bigrams_most_frequent_without_verbs\n",
    "                                                                        (text, num_keywords) for text in \n",
    "                                                                        pymorphy2_lemmatized_corpus_texts]\n",
    "    #4\n",
    "    pymorphy2_predicted_keywords_tf_idf = get_keywords_tf_idf(pymorphy2_lemmatized_corpus_texts, num_keywords)\n",
    "    #5\n",
    "    pymorphy2_predicted_keywords_graph_degree_centrality = [get_keywords_graph_degree_centrality(text, num_keywords)\n",
    "                                                            for text in pymorphy2_lemmatized_corpus_texts]\n",
    "    #6\n",
    "    pymorphy2_predicted_keywords_graph_betweenness_centrality = [get_keywords_graph_betweenness_centrality\n",
    "                                                                 (text, num_keywords) for text in \n",
    "                                                                 pymorphy2_lemmatized_corpus_texts]\n",
    "    #7 \n",
    "    mystem_predicted_keywords_most_frequent = [get_keywords_most_frequent(text, num_keywords) \n",
    "                                               for text in mystem_lemmatized_corpus_texts]\n",
    "    #8\n",
    "    mystem_predicted_keywords_bigrams_most_frequent = [get_keywords_bigrams_most_frequent(text, num_keywords) \n",
    "                                                       for text in mystem_lemmatized_corpus_texts]\n",
    "    #9\n",
    "    mystem_predicted_keywords_bigrams_most_frequent_without_verbs = [get_keywords_bigrams_most_frequent_without_verbs\n",
    "                                                                     (text, num_keywords)\n",
    "                                                                     for text in mystem_lemmatized_corpus_texts]\n",
    "    #10\n",
    "    mystem_predicted_keywords_tf_idf = get_keywords_tf_idf(mystem_lemmatized_corpus_texts, num_keywords)\n",
    "    #11\n",
    "    mystem_predicted_keywords_graph_degree_centrality = [get_keywords_graph_degree_centrality(text, num_keywords) \n",
    "                                                         for text in mystem_lemmatized_corpus_texts]\n",
    "    #12\n",
    "    mystem_predicted_keywords_graph_betweenness_centrality = [get_keywords_graph_betweenness_centrality\n",
    "                                                              (text, num_keywords) for text in \n",
    "                                                              mystem_lemmatized_corpus_texts]\n",
    "    #13\n",
    "    predicted_keywords_bigrams_tf_idf = get_keywords_bigrams_tf_idf(corpus_texts, num_keywords)\n",
    "    return pymorphy2_predicted_keywords_most_frequent, \\\n",
    "           pymorphy2_predicted_keywords_bigrams_most_frequent,\\\n",
    "           pymorphy2_predicted_keywords_bigrams_most_frequent_without_verbs, \\\n",
    "           pymorphy2_predicted_keywords_tf_idf, \\\n",
    "           pymorphy2_predicted_keywords_graph_degree_centrality,\\\n",
    "           pymorphy2_predicted_keywords_graph_betweenness_centrality,\\\n",
    "           mystem_predicted_keywords_most_frequent,\\\n",
    "           mystem_predicted_keywords_bigrams_most_frequent,\\\n",
    "           mystem_predicted_keywords_bigrams_most_frequent_without_verbs,\\\n",
    "           mystem_predicted_keywords_tf_idf,\\\n",
    "           mystem_predicted_keywords_graph_degree_centrality,\\\n",
    "           mystem_predicted_keywords_graph_betweenness_centrality,\\\n",
    "           predicted_keywords_bigrams_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:59:13.945223Z",
     "start_time": "2020-04-27T09:39:35.704283Z"
    },
    "id": "6FBJCrioHk-H"
   },
   "outputs": [],
   "source": [
    "pymorphy2_predicted_keywords_most_frequent, \\\n",
    "pymorphy2_predicted_keywords_bigrams_most_frequent,\\\n",
    "pymorphy2_predicted_keywords_bigrams_most_frequent_without_verbs, \\\n",
    "pymorphy2_predicted_keywords_tf_idf, \\\n",
    "pymorphy2_predicted_keywords_graph_degree_centrality,\\\n",
    "pymorphy2_predicted_keywords_graph_betweenness_centrality,\\\n",
    "mystem_predicted_keywords_most_frequent,\\\n",
    "mystem_predicted_keywords_bigrams_most_frequent,\\\n",
    "mystem_predicted_keywords_bigrams_most_frequent_without_verbs,\\\n",
    "mystem_predicted_keywords_tf_idf,\\\n",
    "mystem_predicted_keywords_graph_degree_centrality,\\\n",
    "mystem_predicted_keywords_graph_betweenness_centrality,\\\n",
    "predicted_keywords_bigrams_tf_idf = get_all_methods_keywords(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgyloBQbHk-K"
   },
   "source": [
    "Чтобы получить названия вызываемых функций для наглядности рейтинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T10:36:02.239390Z",
     "start_time": "2020-04-27T10:36:02.234372Z"
    },
    "id": "UCigJ0EUHk-L"
   },
   "outputs": [],
   "source": [
    "from inspect import currentframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T10:36:03.461560Z",
     "start_time": "2020-04-27T10:36:03.455074Z"
    },
    "id": "qfpEsgwQHk-R"
   },
   "outputs": [],
   "source": [
    "def get_name_of_function_returned_var(var):\n",
    "    \"\"\"берет переменную и возвращает название функции, которая ее произвела\"\"\"\n",
    "    callers_local_vars = currentframe().f_back.f_back.f_locals.items()\n",
    "    name_of_function_returned_var = [var_name for var_name, var_val in callers_local_vars if var_val is var][0]\n",
    "    return name_of_function_returned_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o581eBk4Hk-T"
   },
   "source": [
    "Функция оценки качества предсказания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T10:36:04.323574Z",
     "start_time": "2020-04-27T10:36:04.309608Z"
    },
    "id": "BBmI3TplHk-U"
   },
   "outputs": [],
   "source": [
    "def get_evaluation(manual_keywords, predicted_keywords):\n",
    "    \"\"\"\n",
    "    функция оценки, берет списки наборов эталонных ключевых слов и предсказанных\n",
    "    ключевых слов, возвращает название метода и в кортеже его точность, \n",
    "    полноту, F-меру и коэффициент Жаккара предсказания \n",
    "    \"\"\"\n",
    "    method_name = get_name_of_function_returned_var(predicted_keywords)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    intersections = []\n",
    "    for index, one_text_manual_keywords in enumerate(manual_keywords):\n",
    "        one_text_predicted_keywords = predicted_keywords[index]\n",
    "        intersection = len(set(one_text_manual_keywords) & set(one_text_predicted_keywords))\n",
    "        intersections.append(intersection)\n",
    "        recalls.append(intersection / len(one_text_manual_keywords)) \n",
    "        precisions.append(intersection / len(one_text_predicted_keywords))\n",
    "    num_manual_keywords = sum(len(one_text_manual_keywords) \n",
    "                              for one_text_manual_keywords in manual_keywords)\n",
    "    num_predicted_keywords = sum(len(one_text_predicted_keywords) \n",
    "                                 for one_text_predicted_keywords in predicted_keywords)\n",
    "    num_intersections = sum(intersections) \n",
    "    mean_precision = sum(precisions) / len(precisions)\n",
    "    mean_recall = sum(recalls) / len(recalls)\n",
    "    fmeasure =  (2 * mean_recall * mean_precision) / (mean_recall + mean_precision)\n",
    "    jaccard_index =  num_intersections / (num_manual_keywords + num_predicted_keywords + num_intersections)\n",
    "    return method_name, (mean_precision, mean_recall, fmeasure, jaccard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNx7n2zQHk-Z"
   },
   "source": [
    "Применяем функцию оценки на всех полученных списках ключевых слов (по 13 наборов ключевых слов для каждого текста корпуса)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T10:36:05.439984Z",
     "start_time": "2020-04-27T10:36:05.413053Z"
    },
    "id": "1XTxIN43Hk-Z"
   },
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "#1 \n",
    "evaluations.append(get_evaluation(manual_keywords, pymorphy2_predicted_keywords_most_frequent))\n",
    "#2\n",
    "evaluations.append(get_evaluation(manual_keywords, pymorphy2_predicted_keywords_bigrams_most_frequent))\n",
    "#3\n",
    "evaluations.append(get_evaluation(manual_keywords, pymorphy2_predicted_keywords_bigrams_most_frequent_without_verbs))\n",
    "#4\n",
    "evaluations.append(get_evaluation(manual_keywords, pymorphy2_predicted_keywords_tf_idf))\n",
    "#5\n",
    "evaluations.append(get_evaluation(manual_keywords, pymorphy2_predicted_keywords_graph_degree_centrality))\n",
    "#6\n",
    "evaluations.append(get_evaluation(manual_keywords, pymorphy2_predicted_keywords_graph_betweenness_centrality))\n",
    "#7\n",
    "evaluations.append(get_evaluation(manual_keywords, mystem_predicted_keywords_most_frequent))\n",
    "#8\n",
    "evaluations.append(get_evaluation(manual_keywords, mystem_predicted_keywords_bigrams_most_frequent))\n",
    "#9\n",
    "evaluations.append(get_evaluation(manual_keywords, mystem_predicted_keywords_bigrams_most_frequent_without_verbs))\n",
    "#10\n",
    "evaluations.append(get_evaluation(manual_keywords, mystem_predicted_keywords_tf_idf))\n",
    "#11\n",
    "evaluations.append(get_evaluation(manual_keywords, mystem_predicted_keywords_graph_degree_centrality))\n",
    "#12\n",
    "evaluations.append(get_evaluation(manual_keywords, mystem_predicted_keywords_graph_betweenness_centrality))\n",
    "#13\n",
    "evaluations.append(get_evaluation(manual_keywords, predicted_keywords_bigrams_tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79xrk7YwHk-b"
   },
   "source": [
    "Отсортируем оценки по некоторым аспектам, для этого напишем функцию выдачи рейтинга по номеру аспекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T00:06:03.801395Z",
     "start_time": "2020-04-28T00:06:03.790427Z"
    },
    "id": "zOm_iPoPHk-b"
   },
   "outputs": [],
   "source": [
    "def get_aspect_rating(evaluations, aspect_number):\n",
    "    \"\"\"\n",
    "    берет список оценок в формате dict.items() \n",
    "    и номер аспекта, где\n",
    "    0 -- точность, 1 -- полнота,\n",
    "    2 -- F-мера, 3 -- коэффициент Жаккара,\n",
    "    и врзвращает рейтинг способов по убыванию оценки этого аспекта\n",
    "    \"\"\"\n",
    "    rating = sorted(evaluations, key=lambda item: item[1][aspect_number], reverse=True)\n",
    "    best = rating[0][0]\n",
    "    worst = rating[-1][0]\n",
    "    rating_to_print = []\n",
    "    for index, item in enumerate(rating):\n",
    "        mean_precision, mean_recall, fmeasure, jaccard_index = item[1]\n",
    "        rating_to_print.append(f'{index + 1}. {item[0]}'\n",
    "                               f'\\nТочность: {mean_precision}\\nПолнота: {mean_recall}\\n'\n",
    "                               f'F-мера: {fmeasure}\\nКоэффициент Жаккара: {jaccard_index}\\n')\n",
    "    rating_to_print = '\\n'.join(rating_to_print)\n",
    "    return rating_to_print, best, worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T00:06:04.965283Z",
     "start_time": "2020-04-28T00:06:04.957305Z"
    },
    "id": "wCCdhXzmHk-e"
   },
   "outputs": [],
   "source": [
    "rating_to_print, best, worst = get_aspect_rating(evaluations, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3KW4DvcHk-h"
   },
   "source": [
    "Рейтинг способов по F-мере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T00:06:16.826365Z",
     "start_time": "2020-04-28T00:06:16.820379Z"
    },
    "id": "zbkEnrmAHk-i",
    "outputId": "06f97d29-0f00-4e50-969c-d64284b66ccf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. pymorphy2_predicted_keywords_graph_degree_centrality\n",
      "Точность: 0.1567278287461774\n",
      "Полнота: 0.19446103630911435\n",
      "F-мера: 0.1735673253288406\n",
      "Коэффициент Жаккара: 0.0763615946097698\n",
      "\n",
      "2. mystem_predicted_keywords_graph_degree_centrality\n",
      "Точность: 0.15405198776758414\n",
      "Полнота: 0.19190860725169337\n",
      "F-мера: 0.17090907370641228\n",
      "Коэффициент Жаккара: 0.07514992503748126\n",
      "\n",
      "3. pymorphy2_predicted_keywords_most_frequent\n",
      "Точность: 0.1521406727828746\n",
      "Полнота: 0.18837074452582325\n",
      "F-мера: 0.16832828708817393\n",
      "Коэффициент Жаккара: 0.0742824985931345\n",
      "\n",
      "4. mystem_predicted_keywords_most_frequent\n",
      "Точность: 0.14984709480122324\n",
      "Полнота: 0.18702227863226484\n",
      "F-мера: 0.1663834549903541\n",
      "Коэффициент Жаккара: 0.07323943661971831\n",
      "\n",
      "5. pymorphy2_predicted_keywords_graph_betweenness_centrality\n",
      "Точность: 0.14487767584097855\n",
      "Полнота: 0.17967175985369196\n",
      "F-мера: 0.16040962712595944\n",
      "Коэффициент Жаккара: 0.07097138554216867\n",
      "\n",
      "6. mystem_predicted_keywords_bigrams_most_frequent_without_verbs\n",
      "Точность: 0.14373088685015292\n",
      "Полнота: 0.17884827583730592\n",
      "F-мера: 0.15937806449465466\n",
      "Коэффициент Жаккара: 0.07044641175362591\n",
      "\n",
      "7. mystem_predicted_keywords_graph_betweenness_centrality\n",
      "Точность: 0.14067278287461774\n",
      "Полнота: 0.17634603164534093\n",
      "F-мера: 0.1565023013413844\n",
      "Коэффициент Жаккара: 0.0690435766836446\n",
      "\n",
      "8. pymorphy2_predicted_keywords_bigrams_most_frequent_without_verbs\n",
      "Точность: 0.14105504587155968\n",
      "Полнота: 0.17565465035998185\n",
      "F-мера: 0.15646489551090947\n",
      "Коэффициент Жаккара: 0.06921916258015844\n",
      "\n",
      "9. mystem_predicted_keywords_bigrams_most_frequent\n",
      "Точность: 0.1375054608999563\n",
      "Полнота: 0.1732086318059003\n",
      "F-мера: 0.15330577728811348\n",
      "Коэффициент Жаккара: 0.06777421181801019\n",
      "\n",
      "10. pymorphy2_predicted_keywords_bigrams_most_frequent\n",
      "Точность: 0.13635867190913067\n",
      "Полнота: 0.17146373276853308\n",
      "F-мера: 0.15190945509884002\n",
      "Коэффициент Жаккара: 0.06724593879863996\n",
      "\n",
      "11. mystem_predicted_keywords_tf_idf\n",
      "Точность: 0.13570336391437304\n",
      "Полнота: 0.1666450095644299\n",
      "F-мера: 0.14959093787895936\n",
      "Коэффициент Жаккара: 0.06700641751604379\n",
      "\n",
      "12. pymorphy2_predicted_keywords_tf_idf\n",
      "Точность: 0.13532110091743121\n",
      "Полнота: 0.1660789938788094\n",
      "F-мера: 0.14913062523177498\n",
      "Коэффициент Жаккара: 0.06683028129129696\n",
      "\n",
      "13. predicted_keywords_bigrams_tf_idf\n",
      "Точность: 0.1269113149847094\n",
      "Полнота: 0.15693799558544988\n",
      "F-мера: 0.14033648593901382\n",
      "Коэффициент Жаккара: 0.06293838862559242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rating_to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eosPlnI2Hk-m"
   },
   "source": [
    "Для интереса можно посмотреть на первые несколько эталонных, лучших предсказанных и худших предсказанных групп ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T11:06:38.515871Z",
     "start_time": "2020-04-27T11:06:38.508891Z"
    },
    "id": "nORkLfTVHk-n"
   },
   "outputs": [],
   "source": [
    "def print_manual_best_worst_keywords(best, worst, num_keywords_groups):\n",
    "    \"\"\"\n",
    "    берет списки лучших и худших ключевых слов и количество групп ключевых слов,\n",
    "    печатает их вместе с эталонными ключевыми словами\n",
    "    \"\"\"\n",
    "    for one_text_manual, one_text_best, one_text_worst in zip(manual_keywords[:num_keywords_groups], \n",
    "                                                              globals()[best][:num_keywords_groups], \n",
    "                                                              globals()[worst][:num_keywords_groups]):\n",
    "        print(f'Эталонные слова: {one_text_manual}'\n",
    "              f'\\nЛучший метод {best}: {one_text_best}'\n",
    "              f'\\nХудший метод {worst}: {one_text_worst}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T00:06:40.452183Z",
     "start_time": "2020-04-28T00:06:40.447197Z"
    },
    "id": "fXe-HcPUHk-o",
    "outputId": "ed459a72-ab6e-45e6-f179-8ac4d555d703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эталонные слова: ['выборы в сша', 'сша', 'санкции', 'хакеры', 'эксклюзив rt', 'демократическая партия ', 'дональд трамп', 'закон', 'кибератака', 'кибербезопасность', 'политика', 'республиканская партия', 'хиллари клинтон', 'сша']\n",
      "Лучший метод pymorphy2_predicted_keywords_graph_degree_centrality: ['штат', 'россия', 'сша', 'кибератака', 'резолюция', 'иллинойс', 'трамп', 'система', 'сервер', 'избирательный', 'атака', 'отметить']\n",
      "Худший метод predicted_keywords_bigrams_tf_idf: ['штат', 'иллинойс', 'резолюция', 'штат иллинойс', 'избирательный', 'кибератака', 'сервер', 'избирательный комиссия', 'взлом', 'проект резолюция', 'комиссия', 'комиссия штат']\n",
      "\n",
      "Эталонные слова: ['гражданство', 'ес', 'латвия', 'национализм', 'права человека', 'прибалтика', 'россия', 'россияне', 'русофобия ', 'русский язык', 'ссср', 'прибалтика']\n",
      "Лучший метод pymorphy2_predicted_keywords_graph_degree_centrality: ['латвия', 'гражданство', 'негражданин', 'ребёнок', 'латвийский', 'государство', 'право', 'паспорт', 'страна', 'язык', 'русский', 'житель']\n",
      "Худший метод predicted_keywords_bigrams_tf_idf: ['негражданин', 'латвия', 'гражданство', 'латвийский', 'ребёнок', 'вейонис', 'ребёнок негражданин', 'латышский', 'паспорт', 'язык', 'натурализация', 'знание латышский']\n",
      "\n",
      "Эталонные слова: ['антониу гутерреш ', 'василий небензя', 'вооруженный конфликт', 'оон', 'резолюция', 'россия', 'сша', 'сирия', 'совет безопасности оон', 'видео', 'ближний восток', 'ситуация в сирии']\n",
      "Лучший метод pymorphy2_predicted_keywords_graph_degree_centrality: ['сирия', 'резолюция', 'гуманитарный', 'оон', 'террористический', 'сша', 'прекращение', 'россия', 'провокация', 'сторона', 'действие', 'отметить']\n",
      "Худший метод predicted_keywords_bigrams_tf_idf: ['резолюция', 'прекращение', 'гуманитарный', 'сирия', 'оон', 'прекращение боев', 'режим прекращение', 'совбез', 'террористический', 'провокация', 'джабхата', 'прекращение огонь']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_manual_best_worst_keywords(best, worst, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nuLqqQKHk-q"
   },
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-OTkH3FHk-q"
   },
   "source": [
    "1. MyStem работает дольше pymorphy2 в гораздо большей степени чем лучше\n",
    "2. На разных корпусах результаты могут сильно отличаться, поэтому нельзя сказать однозначно, какой способ лучший и худший\n",
    "3. Функции по частотности и по degree centrality очень эффективные при своей простоте и скорости выполнения\n",
    "4. На этом корпусе текстов моя собственная функция (наиболее авторской я считаю \"Ключевые слова и биграммы по частотности и без глаголов\") показала хороший результат, обогнав TF-IDF с биграммами и без\n",
    "5. Идея убирать глаголы из предсказываемых ключевых слов достаточно эффективная\n",
    "6. TF-IDF с биграммами вероятно будет эффективнее, если прикрутить грамматическое согласование и убрать глаголы\n",
    "7. Возможно, разница между лучшим и худшим способом в 3% -- это не так и много"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "751iBgFnHk78",
    "_dv_1xV-Hk8b",
    "SaopXWjWHk8d",
    "oj7CdifBHk8q",
    "CCpmCA5oHk85",
    "utu9-t9iHk9D",
    "BNW5NAUgHk9O",
    "HyhmW5k3Hk9Z",
    "mkbQmAfTHk9j",
    "-fWTvZfYHk9v",
    "2nuLqqQKHk-q"
   ],
   "name": "Lilya_Kazakova_keywords_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "610px",
    "right": "20px",
    "top": "231px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
